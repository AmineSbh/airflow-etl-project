# Projet de Pipeline de DonnÃ©es : Suivi du CAC 40

Ce projet est une application de pipeline de donnÃ©es complÃ¨te, conÃ§ue pour automatiser la collecte, le stockage et la visualisation des cours de la bourse pour les entreprises du CAC 40.

Le pipeline s'exÃ©cute de maniÃ¨re entiÃ¨rement automatisÃ©e grÃ¢ce Ã  Apache Airflow et est entiÃ¨rement conteneurisÃ© avec Docker. Les donnÃ©es sont ensuite affichÃ©es sur un tableau de bord interactif construit avec Streamlit.

[Image d'un schÃ©ma d'architecture de donnÃ©es simple]

## ğŸ¯ Objectif

L'objectif est de crÃ©er un systÃ¨me robuste et automatisÃ© qui :

Extrait les noms des entreprises du CAC 40 depuis WikipÃ©dia.

RÃ©cupÃ¨re les donnÃ©es boursiÃ¨res (prix, variation, etc.) pour chaque entreprise via l'API yfinance.

Transforme et nettoie ces donnÃ©es.

Charge les donnÃ©es propres dans une base de donnÃ©es PostgreSQL.

Affiche ces donnÃ©es sur un tableau de bord en temps rÃ©el.

## ğŸ› ï¸ Architecture

Ce projet est orchestrÃ© par docker-compose et se compose de trois piliers principaux :

Orchestration (Apache Airflow)

Un Scheduler Airflow planifie et dÃ©clenche notre tÃ¢che ETL Ã  intervalles rÃ©guliers (par exemple, 3 fois par jour).

Le Webserver Airflow (accessible sur localhost:8080) vous permet de surveiller, de dÃ©clencher manuellement et de dÃ©boguer vos pipelines (DAGs).

Pipeline ETL (Docker & Python)

Airflow utilise le DockerOperator pour lancer un conteneur basÃ© sur l'image etl_image.

Ce conteneur exÃ©cute le script Python (scripts_etl/extract.py) qui effectue le scraping (avec Playwright) et la collecte des donnÃ©es (yfinance).

Les donnÃ©es sont transformÃ©es (transform.py) puis chargÃ©es (load.py) dans la base de donnÃ©es.

Stockage & Visualisation (PostgreSQL & Streamlit)

Un service PostgreSQL sert de base de donnÃ©es fiable pour stocker toutes les donnÃ©es historiques collectÃ©es.

Un service Streamlit (accessible sur localhost:8501) lit en permanence cette base de donnÃ©es et affiche les informations sur un tableau de bord interactif.

Voici un schÃ©ma simple du flux de donnÃ©es :

[Airflow]       ->   [Conteneur ETL (Docker)]   ->   [PostgreSQL]   <-   [Streamlit]
(Planifie)           (Extrait & Charge)           (Stocke)         (Lit & Affiche)


## ğŸš€ Comment l'utiliser ?

Ce projet est conÃ§u pour Ãªtre lancÃ© avec une seule commande.

PrÃ©requis

Docker

Docker Compose (gÃ©nÃ©ralement inclus avec Docker Desktop)

Configuration (Utilisateurs Linux)

Si vous Ãªtes sous Linux, vous devez crÃ©er un fichier .env Ã  la racine du projet pour Ã©viter les problÃ¨mes de permissions de fichiers avec Airflow.

echo "AIRFLOW_UID=$(id -u)" > .env


(Les utilisateurs macOS et Windows peuvent gÃ©nÃ©ralement ignorer cette Ã©tape).

Lancement

Clonez ce dÃ©pÃ´t.

Ouvrez un terminal Ã  la racine du projet.

ExÃ©cutez la commande suivante :

docker-compose up -d --build


--build : Force la reconstruction de vos images (nÃ©cessaire si vous modifiez le code Python).

-d : Lance les conteneurs en mode "detached" (en arriÃ¨re-plan).

C'est tout ! L'ensemble de l'infrastructure est en cours d'exÃ©cution.

ğŸ–¥ï¸ AccÃ©der aux services

Une fois les conteneurs lancÃ©s (cela peut prendre une minute ou deux la premiÃ¨re fois) :

Tableau de Bord (Streamlit) :

Ouvrez votre navigateur Ã  l'adresse : http://localhost:8501

### Interface Airflow :

Ouvrez votre navigateur Ã  l'adresse : http://localhost:8080

Identifiant : airflow

Mot de passe : airflow

## ğŸ“‚ Structure du projet

.
â”œâ”€â”€ affichage/            # Contient le code du dashboard Streamlit
â”‚   â”œâ”€â”€ Dockerfile        # Instructions pour construire l'image Streamlit
â”‚   â””â”€â”€ dashboard.py      # Le script Python de l'application Streamlit
â”‚
â”œâ”€â”€ dags/                 # Contient les scripts de dÃ©finition des DAGs Airflow
â”‚   â””â”€â”€ etl_dag.py        # Le DAG qui dÃ©finit et planifie notre pipeline ETL
â”‚
â”œâ”€â”€ scripts_etl/          # Le cÅ“ur de notre pipeline ETL
â”‚   â”œâ”€â”€ Dockerfile        # Instructions pour construire l'image 'etl_image'
â”‚   â”œâ”€â”€ requirements.txt  # DÃ©pendances Python pour l'ETL
â”‚   â”œâ”€â”€ extract.py        # Script principal (Scraping & API)
â”‚   â”œâ”€â”€ transform.py      # Script de nettoyage des donnÃ©es
â”‚   â””â”€â”€ load.py           # Script de chargement en base de donnÃ©es
â”‚
â”œâ”€â”€ tests/                # Tests unitaires pour valider l'ETL
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ docker-compose.yml    # Le fichier chef d'orchestre qui assemble tout
â””â”€â”€ README.md             # Ce fichier


## ğŸ’» Pile technologique

Orchestration : Apache Airflow

Conteneurisation : Docker & Docker Compose

Langage : Python

Base de DonnÃ©es : PostgreSQL

Dashboard : Streamlit

Data Scraping : Playwright (principal), Requests (repli)

Data Fetching : Yfinance

Data Processing : Pandas